GPUS: '0'
OUTPUT_DIR: 'output' # automatically change it to OUTPUT_DIR/yaml file name/timestamp /, which will store the model, log, tensorboard, and so on
WORKERS_PARALLEL: 16
WORKERS_DATALOADER: 16

SELECT_MODEL: 'EgoVideoResnet3D'
SELECT_OUTPUT_HEAD: ''

DATASET:
  NAME: 'DemonProfVideoDataset'  
  ROOT_DIR: './data'
  FRAME_STRIDE: 3
  WINDOW_LENGTH: 16
  USE_IMAGE_INPUT: true # true or false
  USE_POSE_INPUT: false # true, false
  VIDEO_CLIP_LEN: 6
  POSE_CLIP_LEN: 16
  IMAGE_DIR: 'takes_image_downscaled_448'
  PADDING_MODE: 'repeat' # 'zero' or 'repeat'
  ANNOTATION_STRIDE_TRAIN: 1
  ANNOTATION_STRIDE_VAL: 1
  ANNOTATION_STRIDE_TRAINVAL: 1
  # MIN_JOINT_NUM: 10 # when annotated joints are less than this, the data will be ignored (@Frozen = 10, if 'USE_IMAGE_MODE' is 'downscaled') 
  USE_ANNOTATION_MODE: 'annotation' # 'annotation' or 'automatic' or 'both' (@Frozen='annotation', if 'USE_IMAGE_MODE' is 'downscaled') 
  ORIGINAL_IMAGE_SIZE:
    - 448
    - 448
  IMAGE_SIZE:
    - 224
    - 224
  USE_POSE_INPUT: false # true, false
  POSE_DIR: null
  SAMPLE_RATE: 5
  TRAIN_REPEATED: 50
  SCALE:   
  DUMMY_SUMISSION_PATh: 'annotations/proficiency_demonstrator_dummy_submission.json'

  TAKE_NUM_TRAIN: 
  TAKE_NUM_VAL: 
  TAKE_NUM_TEST: 


MODEL:
  BACKBONE:
    TYPE: 'vit_b' # 'vit_s', 'r3d_18' #'resnet'
    LOCAL_PRETRAINED_PATH: 'ckpts/vit_mae/vit_b_k710_dl_from_giant.pth'
    NUM_FRAMES: 6
    EMBED_DIM: 768 # 384 for vit_s, or 768 for vit_b
    POOLING: 'avg' # 'avg' or 'max' or 'attn'

  TEMPORAL_FUSION: 
  OUTPUT_HEAD:
      TYPE: 'mlp' # 'transfromer_decoder'
      FEATURE_DIM: 768
      HIDDEN_DIM: 256
      DROPOUT: 0
      NUM_CLASSES: 4

TRAIN:
  FLAG_USE_TRAINVAL_DATASET: false # if true, use train and val dataset to train the model, otherwise use only train dataset
  PRETRAINED_MODEL: 
  SPLIT: 'train' # 'train' or 'trainval'
  BATCH_SIZE: 64 #16
  SHUFFLE: true # false for test image loading
  LOSS_CRITERION: 'CrossEntropyLoss' # 'CrossEntropyLoss', 'FocalLoss'
  CLASS_WEIGHTS: [1.0, 1.0, 1.0, 1.0]
  OPTIMIZER: 'AdamW' # 'Adam' or 'AdamW'
  LR: 0.0005 # origin 0.0005
  LAYER_DECAY: 0.75
  WEIGHT_DECAY: 0.001  # 0.05, 0.01
  LR_MIN: 0.000001 # minimum learning rate for AdamW
  BEGIN_EPOCH: 0
  END_EPOCH: 6
  WARMUP_EPOCH: 1
  SAVE_INTERVAL: 5  # save model every SAVE_INTERVAL epochs
  ANNOTATION_PATH: 'data/annotations/annotations_train.pkl'
  
VAL:
  BATCH_SIZE: 64 #16
  LOSS_CRITERION: 'CrossEntropyLoss' #'MPJPELoss'
  SHUFFLE: false
  ANNOTATION_PATH: 'data/annotations/annotations_val.pkl'

TEST:
  BATCH_SIZE: 64 #16
  SHUFFLE: false
  ANNOTATION_PATH: 'data/annotations/annotations_test.pkl'